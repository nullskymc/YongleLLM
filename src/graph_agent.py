from typing import TypedDict, Annotated
from langchain_neo4j import GraphCypherQAChain
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.prompts import PromptTemplate
from langchain.schema import HumanMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from adapter import llm, graph
import re
import asyncio
from typing import AsyncGenerator
from datetime import datetime

# å®šä¹‰çŠ¶æ€ç±»å‹
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]
    query: str
    search_result: str
    graph_result: str
    final_answer: str
    workflow_steps: list  # ä¸“é—¨ç”¨äºå­˜å‚¨å·¥ä½œæµæ­¥éª¤

# åˆå§‹åŒ–å·¥å…·
search_tool = DuckDuckGoSearchRun()
graph_chain = GraphCypherQAChain.from_llm(
    graph=graph, llm=llm, verbose=True, allow_dangerous_requests=True
)

# 1. æœç´¢å¼•æ“èŠ‚ç‚¹
def search_engine(state):  # ç§»é™¤ç±»å‹æ³¨è§£ï¼Œå…¼å®¹ dict
    """ä½¿ç”¨æœç´¢å¼•æ“è·å–èƒŒæ™¯ä¿¡æ¯"""
    query = state["query"]
    
    # æ„å»ºæœç´¢æŸ¥è¯¢
    search_query = f"{query}"
    print(f"ğŸ” æ­¥éª¤1: æœç´¢å¼•æ“æŸ¥è¯¢ - {search_query}")
    
    try:
        search_result = search_tool.run(search_query)
        print(f"âœ… æœç´¢å®Œæˆ: {search_result[:200]}...")
          # æ·»åŠ æ­¥éª¤ä¿¡æ¯åˆ°å·¥ä½œæµæ­¥éª¤ä¸­
        step_message = {
            "step": 1,
            "name": "æœç´¢å¼•æ“æŸ¥è¯¢",
            "status": "completed",
            "description": f"æ­£åœ¨æœç´¢: {search_query}",
            "result": search_result[:300] + "..." if len(search_result) > 300 else search_result,
            "icon": "ğŸ”"
        }
        
        workflow_steps = state.get("workflow_steps", [])
        workflow_steps.append(step_message)
        
        return {
            "search_result": search_result,
            "workflow_steps": workflow_steps
        }
        
    except Exception as e:
        print(f"âŒ æœç´¢é”™è¯¯: {e}")
          # æ·»åŠ é”™è¯¯æ­¥éª¤ä¿¡æ¯
        step_message = {
            "step": 1,
            "name": "æœç´¢å¼•æ“æŸ¥è¯¢",
            "status": "error",
            "description": f"æœç´¢å¤±è´¥: {e}",
            "result": "",
            "icon": "âŒ"
        }
        
        workflow_steps = state.get("workflow_steps", [])
        workflow_steps.append(step_message)
        
        return {
            "search_result": f"æœç´¢å¤±è´¥: {e}",
            "workflow_steps": workflow_steps
        }

# 2. çŸ¥è¯†å›¾è°±é—®ç­”èŠ‚ç‚¹
def query_knowledge_graph(state):  # ç§»é™¤ç±»å‹æ³¨è§£ï¼Œå…¼å®¹ dict
    """åŸºäºçŸ¥è¯†å›¾è°±å›ç­”é—®é¢˜"""
    query = state["query"]
    search_result = state.get("search_result", "")
    
    print(f"ğŸ§  æ­¥éª¤2: çŸ¥è¯†å›¾è°±æŸ¥è¯¢")
    
    try:
        # ä½¿ç”¨GraphCypherQAChainæŸ¥è¯¢
        result = graph_chain.invoke({"query": query})
        graph_result = result["result"]
        
        print(f"âœ… å›¾è°±æŸ¥è¯¢å®Œæˆ: {graph_result}")
        
        # å¦‚æœç»“æœä¸ºç©ºæˆ–ä¸æ»¡æ„ï¼Œå°è¯•ä¼˜åŒ–æŸ¥è¯¢
        if not graph_result or "I don't know" in graph_result or len(graph_result.strip()) < 10:
            print(f"ğŸ”„ æ­¥éª¤2.1: ä¼˜åŒ–æŸ¥è¯¢è¯­å¥")
            # æå–å…³é”®è¯é‡æ–°æ„å»ºæŸ¥è¯¢
            optimized_query = optimize_graph_query(query)
            print(f"ä¼˜åŒ–åæŸ¥è¯¢: {optimized_query}")
            
            result = graph_chain.invoke({"query": optimized_query})
            graph_result = result["result"]
        
        # æ·»åŠ æ­¥éª¤ä¿¡æ¯
        step_message = {
            "step": 2,
            "name": "çŸ¥è¯†å›¾è°±æŸ¥è¯¢",
            "status": "completed",
            "description": f"æŸ¥è¯¢çŸ¥è¯†å›¾è°±: {query}",
            "result": graph_result,
            "icon": "ğŸ§ "
        }
        
        workflow_steps = state.get("workflow_steps", [])
        workflow_steps.append(step_message)
        
        return {
            "graph_result": graph_result,
            "workflow_steps": workflow_steps
        }
        
    except Exception as e:
        print(f"âŒ å›¾è°±æŸ¥è¯¢é”™è¯¯: {e}")
        
        step_message = {
            "step": 2,
            "name": "çŸ¥è¯†å›¾è°±æŸ¥è¯¢",
            "status": "error",
            "description": f"å›¾è°±æŸ¥è¯¢å¤±è´¥: {e}",
            "result": "",
            "icon": "âŒ"
        }
        
        workflow_steps = state.get("workflow_steps", [])
        workflow_steps.append(step_message)
        
        return {
            "graph_result": f"æŸ¥è¯¢å¤±è´¥: {e}",
            "workflow_steps": workflow_steps
        }

# 3. ç»“æœèåˆèŠ‚ç‚¹ï¼ˆåŒæ­¥ï¼‰
def synthesize_answer(state: AgentState):
    """èåˆæœç´¢ç»“æœå’Œå›¾è°±ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    query = state["query"]
    search_result = state.get("search_result", "")
    graph_result = state.get("graph_result", "")
    
    print(f"ğŸ”„ æ­¥éª¤3: ç»“æœèåˆä¸ç”Ÿæˆç­”æ¡ˆ")
      # ä½¿ç”¨LLMèåˆä¸¤ä¸ªç»“æœ
    synthesis_prompt = PromptTemplate.from_template("""
        è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·é—®é¢˜æä¾›ä¸€ä¸ªå…¨é¢ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š

        ç”¨æˆ·é—®é¢˜: {query}

        æœç´¢å¼•æ“ç»“æœ:
        {search_result}

        çŸ¥è¯†å›¾è°±ç»“æœ:
        {graph_result}

        è¯·ç»¼åˆåˆ†æä¸Šè¿°ä¿¡æ¯ï¼Œæä¾›ä¸€ä¸ªç®€æ´æ˜ç¡®çš„ç­”æ¡ˆã€‚å¦‚æœä¸¤ä¸ªæ¥æºçš„ä¿¡æ¯æœ‰å†²çªï¼Œè¯·æŒ‡å‡ºå¹¶è¯´æ˜ã€‚
        å¦‚æœæŸä¸ªæ¥æºæ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·åªä½¿ç”¨å¦ä¸€ä¸ªæ¥æºçš„ä¿¡æ¯ã€‚
        """)
    
    try:
        formatted_prompt = synthesis_prompt.format(
            query=query,
            search_result=search_result,
            graph_result=graph_result
        )
        response = llm.invoke([HumanMessage(content=formatted_prompt)])
        
        final_answer = response.content
        print(f"âœ… èåˆå®Œæˆ: {final_answer}")
          # æ·»åŠ æ­¥éª¤ä¿¡æ¯
        step_message = {
            "step": 3,
            "name": "ç»“æœèåˆ",
            "status": "completed",
            "description": "èåˆæœç´¢ç»“æœå’ŒçŸ¥è¯†å›¾è°±ç»“æœ",
            "result": final_answer,
            "icon": "ğŸ”„"
        }
        
        workflow_steps = state.get("workflow_steps", [])
        workflow_steps.append(step_message)
        
        return {
            "final_answer": final_answer,
            "workflow_steps": workflow_steps
        }
        
    except Exception as e:
        print(f"âŒ ç»“æœèåˆé”™è¯¯: {e}")
        # é™çº§å¤„ç†ï¼šå¦‚æœèåˆå¤±è´¥ï¼Œä¼˜å…ˆä½¿ç”¨å›¾è°±ç»“æœï¼Œå…¶æ¬¡æ˜¯æœç´¢ç»“æœ
        fallback_answer = ""
        if graph_result and "æŸ¥è¯¢å¤±è´¥" not in graph_result:
            fallback_answer = graph_result
        elif search_result and "æœç´¢å¤±è´¥" not in search_result:
            fallback_answer = "åŸºäºæœç´¢ç»“æœï¼š" + search_result[:500] + "..."
        else:            fallback_answer = "æŠ±æ­‰ï¼Œæ— æ³•è·å–ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        
        step_message = {
            "step": 3,
            "name": "ç»“æœèåˆ",
            "status": "fallback",
            "description": f"èåˆå¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ: {e}",
            "result": fallback_answer,
            "icon": "âš ï¸"
        }
        
        workflow_steps = state.get("workflow_steps", [])
        workflow_steps.append(step_message)
        
        return {
            "final_answer": fallback_answer,
            "workflow_steps": workflow_steps
        }



# æ–°å¢ï¼šç”¨äºæµå¼è¾“å‡ºçš„ç‹¬ç«‹å¼‚æ­¥ç”Ÿæˆå™¨
async def stream_synthesis(state: AgentState) -> AsyncGenerator[dict, None]:
    """
    æµå¼å¤„ç†çš„è¾…åŠ©å‡½æ•°ï¼šèåˆç»“æœå¹¶ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
    å®ƒæ˜¯ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨ï¼Œä¸ä½œä¸ºå›¾èŠ‚ç‚¹ï¼Œä¸“é—¨ç”± run_agent_stream è°ƒç”¨ã€‚
    """
    query = state["query"]
    search_result = state.get("search_result", "")
    graph_result = state.get("graph_result", "")

    synthesis_prompt = PromptTemplate.from_template("""
        è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·é—®é¢˜æä¾›ä¸€ä¸ªå…¨é¢ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š

        ç”¨æˆ·é—®é¢˜: {query}

        æœç´¢å¼•æ“ç»“æœ:
        {search_result}

        çŸ¥è¯†å›¾è°±ç»“æœ:
        {graph_result}

        è¯·ç»¼åˆåˆ†æä¸Šè¿°ä¿¡æ¯ï¼Œæä¾›ä¸€ä¸ªç®€æ´æ˜ç¡®çš„ç­”æ¡ˆã€‚å¦‚æœä¸¤ä¸ªæ¥æºçš„ä¿¡æ¯æœ‰å†²çªï¼Œè¯·æŒ‡å‡ºå¹¶è¯´æ˜ã€‚
        å¦‚æœæŸä¸ªæ¥æºæ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·åªä½¿ç”¨å¦ä¸€ä¸ªæ¥æºçš„ä¿¡æ¯ã€‚
        """)
    
    formatted_prompt = synthesis_prompt.format(
        query=query,
        search_result=search_result,
        graph_result=graph_result
    )

    accumulated_answer = ""
    try:
        # ä½¿ç”¨ astream å®ç°æµå¼å“åº”
        async for chunk in llm.astream([HumanMessage(content=formatted_prompt)]):
            content = chunk.content
            if isinstance(content, str) and content:
                accumulated_answer += content
                yield {
                    "type": "answer_chunk",
                    "content": content,
                    "is_final": False
                }
        
        # æ ‡è®°æµç»“æŸ
        yield { "type": "answer_chunk", "content": "", "is_final": True }

        # æ„å»ºæœ€ç»ˆçš„å·¥ä½œæµæ­¥éª¤
        step_message = {
            "step": 3,
            "name": "ç»“æœèåˆ",
            "status": "completed",
            "description": "èåˆæœç´¢ç»“æœå’ŒçŸ¥è¯†å›¾è°±ç»“æœ",
            "result": accumulated_answer,
            "icon": "ğŸ”„"
        }
        
        workflow_steps = state.get("workflow_steps", [])
        workflow_steps.append(step_message)

        # é€šè¿‡ç‰¹æ®Šç±»å‹ yield æœ€ç»ˆå®Œæ•´æ•°æ®
        yield {
            "type": "final_data",
            "final_answer": accumulated_answer,
            "workflow_steps": workflow_steps
        }

    except Exception as e:
        print(f"âŒ æµå¼ç»“æœèåˆé”™è¯¯: {e}")
        fallback_answer = graph_result or search_result or "æŠ±æ­‰ï¼Œæ— æ³•è·å–ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        
        yield { "type": "answer_chunk", "content": fallback_answer, "is_final": True }

        step_message = {
            "step": 3,
            "name": "ç»“æœèåˆ",
            "status": "fallback",
            "description": f"èåˆå¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ: {e}",
            "result": fallback_answer,
            "icon": "âš ï¸"
        }
        workflow_steps = state.get("workflow_steps", [])
        workflow_steps.append(step_message)
        
        yield {
            "type": "final_data",
            "final_answer": fallback_answer,
            "workflow_steps": workflow_steps
        }


def optimize_graph_query(original_query: str) -> str:
    """ä½¿ç”¨few-shotç¤ºä¾‹ä¼˜åŒ–å›¾è°±æŸ¥è¯¢è¯­å¥"""    
    optimization_prompt = PromptTemplate.from_template("""
        ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç¤ºä¾‹ï¼Œå°†ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢è½¬æ¢ä¸ºæ›´é€‚åˆçŸ¥è¯†å›¾è°±æŸ¥è¯¢çš„æ ¼å¼ã€‚
        ç¤ºä¾‹ï¼š
        åŸå§‹æŸ¥è¯¢ï¼šæœ‰å“ªäº›è¯—è¯æåˆ°äº†æ¹–æ³Šï¼Ÿ
        ä¼˜åŒ–æŸ¥è¯¢ï¼šæœ‰å“ªäº›è¯—è¯æåˆ°äº†æ¹–æ³Šï¼Ÿ
        åŸå§‹æŸ¥è¯¢ï¼šä»€ä¹ˆæ–¹å¿—è®°è½½äº†æ¹–çš„ä¿¡æ¯
        ä¼˜åŒ–æŸ¥è¯¢ï¼šå“ªäº›æ–¹å¿—è®°è½½äº†æ¹–æ³Šä¿¡æ¯ï¼Ÿ
        åŸå§‹æŸ¥è¯¢ï¼šæ–¹å¿—é‡Œæœ‰æ¹–çš„è®°å½•å—
        ä¼˜åŒ–æŸ¥è¯¢ï¼šæ–¹å¿—ä¸­è®°è½½äº†å“ªäº›æ¹–æ³Šï¼Ÿ
        ç°åœ¨è¯·ä¼˜åŒ–ä»¥ä¸‹æŸ¥è¯¢ï¼š        
        åŸå§‹æŸ¥è¯¢ï¼š{original_query}
        ä¼˜åŒ–æŸ¥è¯¢ï¼š
        """)
    
    try:
        formatted_prompt = optimization_prompt.format(original_query=original_query)
        response = llm.invoke([HumanMessage(content=formatted_prompt)])
          # æ­£ç¡®è·å–å“åº”å†…å®¹
        optimized_query = ""
        if hasattr(response, 'content') and response.content:
            optimized_query = str(response.content).strip()
        else:
            optimized_query = str(response).strip()
        
        # å¦‚æœä¼˜åŒ–ç»“æœä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œè¿”å›åŸæŸ¥è¯¢
        if len(optimized_query) < 3:
            return original_query
            
        return optimized_query
        
    except Exception as e:
        print(f"æŸ¥è¯¢ä¼˜åŒ–é”™è¯¯: {e}")
        return original_query

# 4. æ„å»ºå·¥ä½œæµå›¾
def create_workflow():
    """åˆ›å»ºLangGraphå·¥ä½œæµ"""
    workflow = StateGraph(AgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("search_engine", search_engine)
    workflow.add_node("query_knowledge_graph", query_knowledge_graph)
    workflow.add_node("synthesize_answer", synthesize_answer)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("search_engine")
    
    # æ·»åŠ é¡ºåºè¾¹ï¼šæœç´¢ -> å›¾è°±æŸ¥è¯¢ -> ç»“æœèåˆ -> ç»“æŸ
    workflow.add_edge("search_engine", "query_knowledge_graph")
    workflow.add_edge("query_knowledge_graph", "synthesize_answer")
    workflow.add_edge("synthesize_answer", END)
    
    return workflow.compile()

# ä¸»æ‰§è¡Œå‡½æ•°
def run_agent(query: str):
    """è¿è¡Œæ™ºèƒ½é—®ç­”ä»£ç†"""
    app = create_workflow()
    
    initial_state = {
        "messages": [],
        "query": query,
        "search_result": "",
        "graph_result": "",
        "final_answer": "",
        "workflow_steps": []
    }
    
    print(f"\n=== å¼€å§‹å¤„ç†æŸ¥è¯¢: {query} ===")
    
    # è¿è¡Œå·¥ä½œæµ
    result = app.invoke(initial_state)
    
    print(f"\n=== æœ€ç»ˆç­”æ¡ˆ ===")
    print(result["final_answer"])
    
    return result

# æµå¼æ‰§è¡Œå‡½æ•° (é‡æ„)
async def run_agent_stream(query: str) -> AsyncGenerator[dict, None]:
    """è¿è¡Œæ™ºèƒ½é—®ç­”ä»£ç† - æµå¼ç‰ˆæœ¬"""
    
    initial_state: AgentState = {
        "messages": [],
        "query": query,
        "search_result": "",
        "graph_result": "",
        "final_answer": "",
        "workflow_steps": []
    }
    
    print(f"\n=== å¼€å§‹å¤„ç†æŸ¥è¯¢ (æµå¼): {query} ====")
    
    # å‘é€å¼€å§‹ä¿¡å·
    yield {
        "type": "start",
        "message": "å¼€å§‹å¤„ç†æ‚¨çš„é—®é¢˜...",
        "timestamp": datetime.now().isoformat()
    }
    
    try:
        current_state = initial_state
        
        # æ­¥éª¤1: æœç´¢å¼•æ“
        yield { "type": "step", "step": 1, "name": "æœç´¢å¼•æ“æŸ¥è¯¢", "status": "processing", "description": f"æ­£åœ¨æœç´¢: {query}", "icon": "ğŸ”" }
        await asyncio.sleep(0.1)
        search_result_update = search_engine(current_state)
        current_state["search_result"] = search_result_update["search_result"]
        current_state["workflow_steps"] = search_result_update["workflow_steps"]
        yield { "type": "step", "step": 1, "name": "æœç´¢å¼•æ“æŸ¥è¯¢", "status": "completed", "description": "æœç´¢å®Œæˆ", "result": current_state["search_result"][:200] + "...", "icon": "âœ…" }
        
        # æ­¥éª¤2: çŸ¥è¯†å›¾è°±æŸ¥è¯¢
        yield { "type": "step", "step": 2, "name": "çŸ¥è¯†å›¾è°±æŸ¥è¯¢", "status": "processing", "description": "æŸ¥è¯¢çŸ¥è¯†å›¾è°±æ•°æ®åº“...", "icon": "ğŸ§ " }
        await asyncio.sleep(0.1)
        graph_result_update = query_knowledge_graph(current_state)
        current_state["graph_result"] = graph_result_update["graph_result"]
        current_state["workflow_steps"] = graph_result_update["workflow_steps"]
        yield { "type": "step", "step": 2, "name": "çŸ¥è¯†å›¾è°±æŸ¥è¯¢", "status": "completed", "description": "å›¾è°±æŸ¥è¯¢å®Œæˆ", "result": current_state["graph_result"][:200] + "...", "icon": "âœ…" }
        
        # æ­¥éª¤3: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ (æµå¼)
        yield { "type": "step", "step": 3, "name": "ç”Ÿæˆç­”æ¡ˆ", "status": "processing", "description": "æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...", "icon": "âœ¨" }
        
        final_data_received = False
        async for result in stream_synthesis(current_state):
            if result["type"] == "answer_chunk":
                yield result  # ç›´æ¥å°†ç­”æ¡ˆå—æ¨é€ç»™å®¢æˆ·ç«¯
            elif result["type"] == "final_data":
                current_state["final_answer"] = result["final_answer"]
                current_state["workflow_steps"] = result["workflow_steps"]
                final_data_received = True

        if not final_data_received:
             raise Exception("æµå¼åˆæˆæœªèƒ½ç”Ÿæˆæœ€ç»ˆæ•°æ®ã€‚")

        yield { "type": "step", "step": 3, "name": "ç”Ÿæˆç­”æ¡ˆ", "status": "completed", "description": "ç­”æ¡ˆç”Ÿæˆå®Œæˆ", "icon": "âœ…" }
        
        # å‘é€å®Œæˆä¿¡å·
        yield {
            "type": "complete",
            "final_answer": current_state["final_answer"],
            "workflow_steps": current_state["workflow_steps"],
            "search_result": current_state["search_result"],
            "graph_result": current_state["graph_result"]
        }
        
    except Exception as e:
        print(f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
        yield {
            "type": "error",
            "message": f"å¤„ç†å¤±è´¥: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹
    test_queries = [
        "å®‰ä¸œå¿æ‰€åœ¨çš„çœä»½",  # åœ°åæ¨ç†
        "åˆè‚¥å¿—ä¸Šè®°è½½æœ‰å“ªäº›æ¹–ï¼Ÿ",  # çŸ¥è¯†å›¾è°±é—®ç­”
        "å·¢æ¹–åœ¨å“ªé‡Œ",  # åœ°åæ¨ç†
        "æœ‰å“ªäº›è¯—è¯æåˆ°äº†æ¹–æ³Šï¼Ÿ"  # çŸ¥è¯†å›¾è°±é—®ç­”
    ]
    
    for query in test_queries:
        try:
            result = run_agent(query)
            print("\n" + "="*50 + "\n")
        except Exception as e:
            print(f"å¤„ç†æŸ¥è¯¢ '{query}' æ—¶å‡ºé”™: {e}")
            print("\n" + "="*50 + "\n")