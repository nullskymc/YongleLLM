{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c05a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 伪代码和概念结构\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "# 1. 初始化模型和图数据库连接\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,  # 设置温度以控制输出的随机性\n",
    "    base_url=\"\",\n",
    "    api_key=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc7f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsplitter import LLMStructuralTextSplitter,ConcurrentHierarchicalSplitter\n",
    "\n",
    "coarse_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,          # 可以适当增大块大小\n",
    "    chunk_overlap=200,        # <-- 设置一个足够大的重叠区\n",
    "    keep_separator=True,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "fine_splitter = LLMStructuralTextSplitter(llm=llm)\n",
    "\n",
    "concurrent_splitter = ConcurrentHierarchicalSplitter(\n",
    "    coarse_splitter=coarse_splitter,\n",
    "    fine_splitter=fine_splitter,\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71d0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 加载和处理文档\n",
    "# 加载所有方志和诗词的txt文件\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(file_path=\"./data_simplified.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c17342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步骤 1: 正在进行重叠预分块... ---\n",
      "--- 预分块完成，共得到 71 个重叠的巨型块。 ---\n",
      "\n",
      "--- 步骤 2: 正在并发处理所有块 (并发上限: 10)... ---\n",
      "--- 并发处理完成。 ---\n",
      "\n",
      "--- 步骤 3: 正在进行结果的裁剪、去重与错误处理... ---\n",
      "  [警告] 第 29 个块处理失败，错误类型: ContentFilterFinishReasonError。将保留原始粗分块内容。\n",
      "  [警告] 第 51 个块处理失败，错误类型: ContentFilterFinishReasonError。将保留原始粗分块内容。\n",
      "--- 裁剪、去重与错误处理完成。 ---\n"
     ]
    }
   ],
   "source": [
    "docs=concurrent_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79949917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 所有文档块已成功保存到 -> split_outputs_jsonl/processed_chunks2.jsonl ---\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "output_dir = Path(\"split_outputs_jsonl\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_filename = \"processed_chunks2.jsonl\"\n",
    "output_path = output_dir / output_filename\n",
    "\n",
    "# 2. 以写入模式打开文件\n",
    "try:\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # 遍历每一个 Document 对象\n",
    "        for doc in docs:\n",
    "            # a. 将 Document 对象转换为一个字典\n",
    "            data_record = {\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            \n",
    "            # b. 使用 json.dumps 将字典序列化为 JSON 字符串\n",
    "            #    ensure_ascii=False 对于正确保存中文字符至关重要\n",
    "            json_string = json.dumps(data_record, ensure_ascii=False)\n",
    "            \n",
    "            # c. 将 JSON 字符串写入文件，并在末尾加上换行符\n",
    "            f.write(json_string + \"\\n\")\n",
    "\n",
    "    print(f\"--- 所有文档块已成功保存到 -> {output_path} ---\")\n",
    "\n",
    "except IOError as e:\n",
    "    print(f\"--- 写入文件时发生错误: {output_path}. 错误详情: {e} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc9b9a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 开始从文件加载 Document 块: split_outputs_jsonl/processed_chunks2.jsonl ---\n",
      "--- 加载完成！成功从文件恢复了 432 个 Document 对象。 ---\n",
      "\n",
      "验证加载的第一个文档块:\n",
      "内容:\n",
      "---\n",
      "永乐大典卷之二千二百六十八【六模】\n",
      "湖\n",
      "巢湖\n",
      "《合肥志》\n",
      "在合肥县东南六十里。亦名焦湖。汉明帝十一年。漅湖出黄金。庐江太守以献。漅。音勦或曰勦湖。俗讹为焦湖。以其在巢县。亦曰巢湖。周围四百里。港（汊）...\n",
      "---\n",
      "元数据: {'source': './data_simplified.txt'}\n"
     ]
    }
   ],
   "source": [
    "# --- 从 JSONL 文件加载回 Document 列表的代码 ---\n",
    "\n",
    "# 1. 定义要读取的文件路径\n",
    "input_path = Path(\"split_outputs_jsonl/processed_chunks2.jsonl\")\n",
    "\n",
    "# 2. 准备一个空列表来存放加载后的 Document 对象\n",
    "loaded_documents: List[Document] = []\n",
    "\n",
    "print(f\"\\n--- 开始从文件加载 Document 块: {input_path} ---\")\n",
    "\n",
    "# 3. 检查文件是否存在\n",
    "if not input_path.exists():\n",
    "    print(f\"--- 错误：文件不存在 -> {input_path} ---\")\n",
    "else:\n",
    "    try:\n",
    "        # 以读取模式打开文件\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            # 逐行读取文件\n",
    "            for line in f:\n",
    "                # a. 使用 json.loads 将每一行的 JSON 字符串解析为字典\n",
    "                data_record = json.loads(line)\n",
    "                \n",
    "                # b. 使用字典中的数据重新创建 Document 对象\n",
    "                doc = Document(\n",
    "                    page_content=data_record[\"page_content\"],\n",
    "                    metadata=data_record[\"metadata\"]\n",
    "                )\n",
    "                \n",
    "                # c. 将创建好的 Document 对象添加到列表中\n",
    "                loaded_documents.append(doc)\n",
    "\n",
    "        print(f\"--- 加载完成！成功从文件恢复了 {len(loaded_documents)} 个 Document 对象。 ---\")\n",
    "\n",
    "        # 验证一下加载回来的第一个 Document\n",
    "        if loaded_documents:\n",
    "            print(\"\\n验证加载的第一个文档块:\")\n",
    "            first_doc = loaded_documents[0]\n",
    "            print(f\"内容:\\n---\\n{first_doc.page_content[:100]}...\\n---\")\n",
    "            print(f\"元数据: {first_doc.metadata}\")\n",
    "            \n",
    "    except (IOError, json.JSONDecodeError) as e:\n",
    "        print(f\"--- 读取或解析文件时发生错误: {input_path}. 错误详情: {e} ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
